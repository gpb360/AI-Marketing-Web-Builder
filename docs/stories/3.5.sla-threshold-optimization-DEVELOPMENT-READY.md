# Story 3.5: Intelligent SLA Threshold Optimization - DEVELOPMENT READY

## 🚀 **DEVELOPMENT STATUS: READY FOR IMPLEMENTATION**

### **Executive Summary**
This story implements ML-based automatic optimization of SLA thresholds using historical performance data, A/B testing, and predictive analytics to achieve realistic and continuously improving SLA targets.

### **Story Definition**
**As a** DevOps engineer,  
**I want** the system to automatically optimize SLA thresholds based on actual performance patterns,  
**so that** I have realistic and achievable SLA targets that improve over time.

## 🎯 **ACCEPTANCE CRITERIA**

### **AC1: Performance Data Analysis Engine**
- [ ] System analyzes 30-day performance patterns for each service type
- [ ] Statistical significance validation (95% confidence interval)
- [ ] Automated outlier detection and data quality validation
- [ ] Trend analysis with seasonal pattern recognition
- [ ] Performance baseline calculation with confidence intervals

### **AC2: ML-Based Threshold Optimization**
- [ ] Multi-objective optimization balancing reliability vs achievability
- [ ] Machine learning model for threshold recommendations
- [ ] Business impact assessment for proposed changes
- [ ] Continuous learning system with performance feedback
- [ ] Model retraining pipeline (weekly automated updates)

### **AC3: A/B Testing Framework**
- [ ] Controlled threshold experiments with statistical rigor
- [ ] Randomized assignment with control group management
- [ ] Automated performance monitoring during tests
- [ ] Early stopping criteria for performance degradation
- [ ] Statistical significance testing and result analysis

### **AC4: Historical Tracking & Predictive Analytics**
- [ ] Performance trend tracking with improvement visualization
- [ ] Predictive analytics for future SLA performance
- [ ] Forecasting with confidence intervals and scenario analysis
- [ ] Historical threshold change impact assessment
- [ ] Optimization effectiveness reporting

### **AC5: One-Click Threshold Management**
- [ ] One-click threshold updates with preview
- [ ] Rollback capability with automated triggers
- [ ] Change impact assessment before implementation
- [ ] Audit trail for all threshold modifications
- [ ] Integration with existing SLA violation system

### **AC6: Continuous Learning System**
- [ ] ML model tracks actual vs predicted performance
- [ ] Automated model accuracy monitoring
- [ ] Algorithm effectiveness measurement
- [ ] Performance drift detection and alerts
- [ ] Quarterly algorithm evaluation and improvement

### **AC7: Seamless Integration**
- [ ] Maintains existing SLA monitoring functionality
- [ ] Preserves current violation detection and alerting
- [ ] Enhances existing dashboard without disruption
- [ ] Backward compatibility with current SLA configurations
- [ ] API compatibility for existing integrations

## 🏗️ **TECHNICAL ARCHITECTURE**

### **System Overview**
```
┌─────────────────────────────────────────────────────────────┐
│                   SLA Threshold Optimization                │
├─────────────────────────────────────────────────────────────┤
│  Frontend: ThresholdOptimizationPanel.tsx                  │
│  Backend:  ML Services + API Endpoints                     │
│  Data:     Historical Performance + ML Models              │
│  ML:       Scikit-learn + Time Series Analysis            │
└─────────────────────────────────────────────────────────────┘
```

### **Data Flow Architecture**
```
Historical SLA Data → Performance Analysis → ML Optimization →
A/B Testing → Impact Assessment → Threshold Update → Monitoring
```

## 📊 **DATABASE SCHEMA**

### **New Tables Required**

```sql
-- SLA Threshold History
CREATE TABLE sla_threshold_history (
    id SERIAL PRIMARY KEY,
    service_type VARCHAR(100) NOT NULL,
    threshold_value DECIMAL(10,2) NOT NULL,
    threshold_unit VARCHAR(20) NOT NULL DEFAULT 'seconds',
    previous_value DECIMAL(10,2),
    optimization_reason TEXT,
    optimization_confidence DECIMAL(5,4), -- ML model confidence 0-1
    business_justification TEXT,
    implemented_at TIMESTAMP DEFAULT NOW(),
    implemented_by VARCHAR(100),
    rollback_criteria JSONB,
    performance_impact JSONB,
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- Performance Analysis Cache
CREATE TABLE sla_performance_analysis (
    id SERIAL PRIMARY KEY,
    service_type VARCHAR(100) NOT NULL,
    analysis_period_days INTEGER DEFAULT 30,
    analysis_timestamp TIMESTAMP DEFAULT NOW(),
    performance_statistics JSONB NOT NULL, -- mean, median, percentiles, std
    trend_analysis JSONB, -- direction, seasonal patterns, correlations
    baseline_performance JSONB, -- baseline metrics with confidence intervals
    data_quality_score DECIMAL(3,2), -- 0-1 quality score
    sample_size INTEGER,
    confidence_level DECIMAL(3,2) DEFAULT 0.95,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Threshold Optimization Experiments
CREATE TABLE sla_optimization_experiments (
    id SERIAL PRIMARY KEY,
    experiment_name VARCHAR(200) NOT NULL,
    service_type VARCHAR(100) NOT NULL,
    control_threshold DECIMAL(10,2) NOT NULL,
    test_threshold DECIMAL(10,2) NOT NULL,
    hypothesis TEXT,
    expected_improvement DECIMAL(5,2), -- Expected improvement percentage
    
    -- Experiment configuration
    experiment_start TIMESTAMP,
    experiment_end TIMESTAMP,
    duration_days INTEGER DEFAULT 14,
    traffic_split DECIMAL(3,2) DEFAULT 0.5, -- 50% split
    significance_level DECIMAL(3,2) DEFAULT 0.05, -- 95% confidence
    
    -- Results
    results JSONB,
    statistical_significance BOOLEAN,
    p_value DECIMAL(10,8),
    effect_size DECIMAL(10,4),
    confidence_interval JSONB,
    
    -- Status and control
    status VARCHAR(20) DEFAULT 'planning', -- planning, running, completed, stopped
    early_stop_reason TEXT,
    recommendation VARCHAR(20), -- adopt, reject, extend
    rollback_triggered BOOLEAN DEFAULT false,
    
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- ML Model Performance Tracking
CREATE TABLE sla_ml_model_performance (
    id SERIAL PRIMARY KEY,
    model_type VARCHAR(50) NOT NULL, -- threshold_optimizer, impact_predictor
    model_version VARCHAR(20) NOT NULL,
    training_timestamp TIMESTAMP DEFAULT NOW(),
    
    -- Performance metrics
    accuracy_score DECIMAL(5,4),
    precision_score DECIMAL(5,4),
    recall_score DECIMAL(5,4),
    f1_score DECIMAL(5,4),
    mse DECIMAL(10,6), -- Mean Squared Error for regression
    mae DECIMAL(10,6), -- Mean Absolute Error
    
    -- Training details
    training_data_size INTEGER,
    validation_data_size INTEGER,
    feature_count INTEGER,
    hyperparameters JSONB,
    cross_validation_scores JSONB,
    
    -- Model artifacts
    model_path VARCHAR(500), -- Path to saved model file
    feature_importance JSONB,
    
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Threshold Change Impact Predictions
CREATE TABLE sla_threshold_impact_predictions (
    id SERIAL PRIMARY KEY,
    service_type VARCHAR(100) NOT NULL,
    current_threshold DECIMAL(10,2) NOT NULL,
    proposed_threshold DECIMAL(10,2) NOT NULL,
    
    -- Predictions
    predicted_violation_rate_change DECIMAL(5,2), -- Percentage change
    predicted_reliability_impact DECIMAL(5,2),
    predicted_team_stress_change DECIMAL(5,2),
    confidence_score DECIMAL(3,2), -- Model confidence 0-1
    
    -- Risk assessment
    implementation_risk VARCHAR(20), -- low, medium, high
    risk_factors JSONB,
    mitigation_strategies JSONB,
    
    -- Actual outcomes (filled after implementation)
    actual_violation_rate_change DECIMAL(5,2),
    actual_reliability_impact DECIMAL(5,2),
    prediction_accuracy DECIMAL(5,4),
    
    created_at TIMESTAMP DEFAULT NOW(),
    implemented_at TIMESTAMP,
    evaluated_at TIMESTAMP
);

-- Optimize existing SLA Configuration table
ALTER TABLE sla_configurations ADD COLUMN IF NOT EXISTS optimization_enabled BOOLEAN DEFAULT true;
ALTER TABLE sla_configurations ADD COLUMN IF NOT EXISTS auto_adjust BOOLEAN DEFAULT false;
ALTER TABLE sla_configurations ADD COLUMN IF NOT EXISTS min_threshold DECIMAL(10,2);
ALTER TABLE sla_configurations ADD COLUMN IF NOT EXISTS max_threshold DECIMAL(10,2);
ALTER TABLE sla_configurations ADD COLUMN IF NOT EXISTS optimization_frequency VARCHAR(20) DEFAULT 'weekly';
ALTER TABLE sla_configurations ADD COLUMN IF NOT EXISTS last_optimization_at TIMESTAMP;
```

## 🛠️ **BACKEND IMPLEMENTATION**

### **Core Services Architecture**

#### **1. SLA Performance Analysis Service**
```python
# backend/app/services/sla_performance_analysis_service.py

from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
import numpy as np
from scipy import stats
from sklearn.preprocessing import StandardScaler
import pandas as pd
from app.models.workflow import SLAConfiguration, WorkflowPerformanceMetric
from app.core.database import get_db

class SLAPerformanceAnalysisService:
    """
    Analyzes historical SLA performance data for optimization recommendations.
    """
    
    def __init__(self):
        self.confidence_level = 0.95
        self.min_sample_size = 100
        
    async def analyze_service_performance(
        self, 
        service_type: str, 
        analysis_period_days: int = 30
    ) -> Dict[str, Any]:
        """
        Perform comprehensive performance analysis for a service type.
        
        Returns:
            {
                'service_type': str,
                'analysis_period': int,
                'sample_size': int,
                'data_quality_score': float,
                'performance_statistics': {
                    'mean': float,
                    'median': float,
                    'std': float,
                    'percentile_95': float,
                    'percentile_99': float,
                    'coefficient_variation': float
                },
                'trend_analysis': {
                    'direction': 'improving|degrading|stable',
                    'trend_slope': float,
                    'seasonal_patterns': List[Dict],
                    'load_correlations': List[Dict]
                },
                'baseline_performance': {
                    'baseline_value': float,
                    'confidence_interval_lower': float,
                    'confidence_interval_upper': float,
                    'is_stable': bool
                },
                'outliers': {
                    'count': int,
                    'percentage': float,
                    'values': List[float]
                }
            }
        """
        
        # Get historical performance data
        performance_data = await self._get_performance_data(service_type, analysis_period_days)
        
        if len(performance_data) < self.min_sample_size:
            raise ValueError(f"Insufficient data: {len(performance_data)} samples, minimum {self.min_sample_size} required")
        
        # Calculate basic statistics
        performance_stats = self._calculate_performance_statistics(performance_data)
        
        # Trend analysis
        trend_analysis = self._analyze_trends(performance_data)
        
        # Baseline calculation
        baseline_performance = self._calculate_baseline_performance(performance_data)
        
        # Outlier detection
        outliers = self._detect_outliers(performance_data)
        
        # Data quality assessment
        data_quality_score = self._assess_data_quality(performance_data, outliers)
        
        analysis_result = {
            'service_type': service_type,
            'analysis_period': analysis_period_days,
            'sample_size': len(performance_data),
            'data_quality_score': data_quality_score,
            'performance_statistics': performance_stats,
            'trend_analysis': trend_analysis,
            'baseline_performance': baseline_performance,
            'outliers': outliers
        }
        
        # Cache analysis result
        await self._cache_analysis_result(analysis_result)
        
        return analysis_result
    
    def _calculate_performance_statistics(self, data: List[float]) -> Dict[str, float]:
        """Calculate comprehensive performance statistics."""
        data_array = np.array(data)
        
        return {
            'mean': float(np.mean(data_array)),
            'median': float(np.median(data_array)),
            'std': float(np.std(data_array)),
            'percentile_95': float(np.percentile(data_array, 95)),
            'percentile_99': float(np.percentile(data_array, 99)),
            'coefficient_variation': float(np.std(data_array) / np.mean(data_array))
        }
    
    def _analyze_trends(self, data: List[Dict]) -> Dict[str, Any]:
        """Analyze performance trends and patterns."""
        # Convert to time series
        df = pd.DataFrame(data)
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        df = df.sort_values('timestamp')
        
        # Trend analysis using linear regression
        x = np.arange(len(df))
        slope, intercept, r_value, p_value, std_err = stats.linregress(x, df['value'])
        
        # Determine trend direction
        if p_value < 0.05:  # Statistically significant trend
            if slope > 0:
                direction = 'degrading'  # Performance getting worse (higher times)
            else:
                direction = 'improving'
        else:
            direction = 'stable'
        
        # Seasonal pattern detection (daily, weekly patterns)
        seasonal_patterns = self._detect_seasonal_patterns(df)
        
        # Load correlation analysis
        load_correlations = self._analyze_load_correlations(df)
        
        return {
            'direction': direction,
            'trend_slope': slope,
            'trend_confidence': 1 - p_value,
            'seasonal_patterns': seasonal_patterns,
            'load_correlations': load_correlations
        }
    
    def _calculate_baseline_performance(self, data: List[float]) -> Dict[str, Any]:
        """Calculate performance baseline with confidence intervals."""
        data_array = np.array(data)
        
        # Calculate confidence interval for mean
        confidence_interval = stats.t.interval(
            self.confidence_level,
            len(data_array) - 1,
            loc=np.mean(data_array),
            scale=stats.sem(data_array)
        )
        
        # Stability assessment (coefficient of variation < 0.3 is considered stable)
        cv = np.std(data_array) / np.mean(data_array)
        is_stable = cv < 0.3
        
        return {
            'baseline_value': float(np.mean(data_array)),
            'confidence_interval_lower': float(confidence_interval[0]),
            'confidence_interval_upper': float(confidence_interval[1]),
            'coefficient_variation': cv,
            'is_stable': is_stable
        }
    
    def _detect_outliers(self, data: List[float]) -> Dict[str, Any]:
        """Detect outliers using IQR method."""
        data_array = np.array(data)
        
        Q1 = np.percentile(data_array, 25)
        Q3 = np.percentile(data_array, 75)
        IQR = Q3 - Q1
        
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        outliers = data_array[(data_array < lower_bound) | (data_array > upper_bound)]
        
        return {
            'count': len(outliers),
            'percentage': (len(outliers) / len(data_array)) * 100,
            'values': outliers.tolist(),
            'lower_bound': lower_bound,
            'upper_bound': upper_bound
        }
    
    async def _get_performance_data(self, service_type: str, days: int) -> List[Dict]:
        """Retrieve historical performance data from database."""
        # Implementation to query WorkflowPerformanceMetric table
        # Filter by service type and time range
        pass
    
    async def _cache_analysis_result(self, analysis: Dict[str, Any]) -> None:
        """Cache analysis result in database."""
        # Implementation to store in sla_performance_analysis table
        pass
```

#### **2. SLA Threshold Optimizer Service**
```python
# backend/app/services/sla_threshold_optimizer.py

from typing import Dict, Any, List, Tuple
import numpy as np
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, mean_absolute_error
from scipy.optimize import minimize
import joblib
from datetime import datetime

class SLAThresholdOptimizer:
    """
    ML-based SLA threshold optimization using multi-objective optimization.
    """
    
    def __init__(self):
        self.model = None
        self.feature_scaler = StandardScaler()
        self.target_scaler = StandardScaler()
        self.model_version = "1.0"
        
    async def optimize_thresholds(
        self, 
        performance_analysis: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Generate optimized threshold recommendations.
        
        Args:
            performance_analysis: Output from SLAPerformanceAnalysisService
            
        Returns:
            {
                'service_type': str,
                'current_threshold': float,
                'recommended_threshold': float,
                'optimization_rationale': {
                    'statistical_basis': str,
                    'reliability_impact': str,
                    'achievability_improvement': str,
                    'business_justification': str
                },
                'predicted_outcomes': {
                    'expected_violation_rate': float,
                    'reliability_score_change': float,
                    'team_stress_reduction': float
                },
                'confidence_score': float,
                'implementation_risk': str,
                'rollback_criteria': List[str]
            }
        """
        
        service_type = performance_analysis['service_type']
        
        # Get current threshold
        current_threshold = await self._get_current_threshold(service_type)
        
        # Prepare features for ML model
        features = self._prepare_features(performance_analysis)
        
        # Load or train optimization model
        if not self.model:
            await self._load_or_train_model()
        
        # Multi-objective optimization
        optimal_threshold = self._multi_objective_optimization(
            current_threshold=current_threshold,
            features=features,
            performance_stats=performance_analysis['performance_statistics']
        )
        
        # Predict outcomes
        predicted_outcomes = await self._predict_threshold_impact(
            service_type=service_type,
            current_threshold=current_threshold,
            proposed_threshold=optimal_threshold,
            features=features
        )
        
        # Generate rationale
        rationale = self._generate_optimization_rationale(
            performance_analysis, current_threshold, optimal_threshold
        )
        
        # Risk assessment
        risk_assessment = self._assess_implementation_risk(
            current_threshold, optimal_threshold, predicted_outcomes
        )
        
        # Model confidence
        confidence_score = self._calculate_confidence_score(features)
        
        return {
            'service_type': service_type,
            'current_threshold': current_threshold,
            'recommended_threshold': optimal_threshold,
            'optimization_rationale': rationale,
            'predicted_outcomes': predicted_outcomes,
            'confidence_score': confidence_score,
            'implementation_risk': risk_assessment['risk_level'],
            'rollback_criteria': risk_assessment['rollback_criteria']
        }
    
    def _multi_objective_optimization(
        self, 
        current_threshold: float, 
        features: np.ndarray,
        performance_stats: Dict[str, float]
    ) -> float:
        """
        Multi-objective optimization balancing reliability vs achievability.
        
        Objectives:
        1. Minimize violation rate (reliability)
        2. Maximize achievability (threshold not too tight)
        3. Minimize change magnitude (stability)
        """
        
        def objective_function(threshold: float) -> float:
            # Reliability objective: violation rate
            violation_rate = self._predict_violation_rate(threshold, features)
            
            # Achievability objective: how often can we meet this threshold
            achievability = self._calculate_achievability(threshold, performance_stats)
            
            # Stability objective: minimize change from current
            change_magnitude = abs(threshold - current_threshold) / current_threshold
            
            # Weighted combination
            weights = {'reliability': 0.4, 'achievability': 0.4, 'stability': 0.2}
            
            return (
                weights['reliability'] * violation_rate +
                weights['achievability'] * (1 - achievability) +
                weights['stability'] * change_magnitude
            )
        
        # Optimization bounds (±50% of current threshold)
        bounds = [
            (current_threshold * 0.5, current_threshold * 1.5)
        ]
        
        result = minimize(
            objective_function,
            x0=[current_threshold],
            bounds=bounds,
            method='L-BFGS-B'
        )
        
        return float(result.x[0])
    
    def _predict_violation_rate(self, threshold: float, features: np.ndarray) -> float:
        """Predict violation rate for given threshold."""
        # Add threshold to features
        threshold_features = np.append(features, threshold)
        threshold_features = self.feature_scaler.transform([threshold_features])
        
        # Predict using trained model
        violation_rate = self.model.predict(threshold_features)[0]
        return max(0.0, min(1.0, violation_rate))  # Clamp to [0, 1]
    
    def _calculate_achievability(self, threshold: float, performance_stats: Dict) -> float:
        """Calculate achievability score (how often we can meet this threshold)."""
        # Use percentile-based achievability
        # If threshold is at 95th percentile, achievability is 95%
        percentiles = [90, 95, 99]
        percentile_values = [
            performance_stats['percentile_95'],  # Using 95th as proxy for 90th
            performance_stats['percentile_95'],
            performance_stats['percentile_99']
        ]
        
        # Interpolate achievability based on threshold position
        achievability = np.interp(threshold, percentile_values, [0.9, 0.95, 0.99])
        return min(1.0, achievability)
    
    async def _load_or_train_model(self) -> None:
        """Load existing model or train new one if needed."""
        try:
            # Try to load existing model
            self.model = joblib.load(f'/app/ml_models/sla_threshold_optimizer_{self.model_version}.joblib')
            self.feature_scaler = joblib.load(f'/app/ml_models/sla_feature_scaler_{self.model_version}.joblib')
        except FileNotFoundError:
            # Train new model
            await self._train_optimization_model()
    
    async def _train_optimization_model(self) -> None:
        """Train the threshold optimization model."""
        # Get training data from historical threshold changes
        training_data = await self._get_training_data()
        
        if len(training_data) < 50:  # Minimum training data
            # Use default heuristic-based optimization
            self.model = self._create_heuristic_model()
            return
        
        # Prepare features and targets
        X, y = self._prepare_training_data(training_data)
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # Scale features
        X_train_scaled = self.feature_scaler.fit_transform(X_train)
        X_test_scaled = self.feature_scaler.transform(X_test)
        
        # Train model (ensemble of Random Forest and Gradient Boosting)
        rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
        gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)
        
        rf_model.fit(X_train_scaled, y_train)
        gb_model.fit(X_train_scaled, y_train)
        
        # Ensemble prediction
        self.model = EnsembleOptimizationModel(rf_model, gb_model)
        
        # Evaluate model
        train_score = self.model.score(X_train_scaled, y_train)
        test_score = self.model.score(X_test_scaled, y_test)
        
        # Save model performance metrics
        await self._save_model_performance({
            'model_type': 'threshold_optimizer',
            'model_version': self.model_version,
            'train_score': train_score,
            'test_score': test_score,
            'training_data_size': len(X_train),
            'validation_data_size': len(X_test)
        })
        
        # Save model
        joblib.dump(self.model, f'/app/ml_models/sla_threshold_optimizer_{self.model_version}.joblib')
        joblib.dump(self.feature_scaler, f'/app/ml_models/sla_feature_scaler_{self.model_version}.joblib')

class EnsembleOptimizationModel:
    """Ensemble model combining Random Forest and Gradient Boosting."""
    
    def __init__(self, rf_model, gb_model):
        self.rf_model = rf_model
        self.gb_model = gb_model
        
    def predict(self, X):
        rf_pred = self.rf_model.predict(X)
        gb_pred = self.gb_model.predict(X)
        # Weighted average (RF: 60%, GB: 40%)
        return 0.6 * rf_pred + 0.4 * gb_pred
    
    def score(self, X, y):
        predictions = self.predict(X)
        return 1 - mean_squared_error(y, predictions)
```

#### **3. A/B Testing Service**
```python
# backend/app/services/sla_ab_testing_service.py

import asyncio
import uuid
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
from scipy import stats
import numpy as np
from app.models.workflow import SLAConfiguration

class SLAABTestingService:
    """
    A/B testing framework for SLA threshold optimization.
    """
    
    def __init__(self):
        self.min_sample_size = 100
        self.significance_level = 0.05
        self.power = 0.8
        
    async def create_threshold_experiment(
        self,
        service_type: str,
        current_threshold: float,
        proposed_threshold: float,
        experiment_duration_days: int = 14,
        traffic_split: float = 0.5
    ) -> Dict[str, Any]:
        """
        Create a new A/B test for threshold optimization.
        
        Returns:
            {
                'experiment_id': str,
                'service_type': str,
                'control_threshold': float,
                'test_threshold': float,
                'traffic_split': float,
                'estimated_duration_days': int,
                'required_sample_size': int,
                'power_analysis': Dict[str, float],
                'experiment_config': Dict[str, Any]
            }
        """
        
        # Power analysis for sample size calculation
        power_analysis = self._calculate_required_sample_size(
            current_threshold=current_threshold,
            proposed_threshold=proposed_threshold,
            service_type=service_type
        )
        
        # Generate experiment configuration
        experiment_id = str(uuid.uuid4())
        
        experiment_config = {
            'experiment_id': experiment_id,
            'service_type': service_type,
            'control_threshold': current_threshold,
            'test_threshold': proposed_threshold,
            'traffic_split': traffic_split,
            'significance_level': self.significance_level,
            'power': self.power,
            'start_time': datetime.utcnow(),
            'estimated_end_time': datetime.utcnow() + timedelta(days=experiment_duration_days),
            'status': 'planning'
        }
        
        # Save experiment to database
        await self._save_experiment_config(experiment_config, power_analysis)
        
        # Setup monitoring
        await self._setup_experiment_monitoring(experiment_id)
        
        return {
            'experiment_id': experiment_id,
            'service_type': service_type,
            'control_threshold': current_threshold,
            'test_threshold': proposed_threshold,
            'traffic_split': traffic_split,
            'estimated_duration_days': experiment_duration_days,
            'required_sample_size': power_analysis['required_sample_size'],
            'power_analysis': power_analysis,
            'experiment_config': experiment_config
        }
    
    async def start_experiment(self, experiment_id: str) -> Dict[str, Any]:
        """Start the A/B test experiment."""
        
        # Update experiment status
        await self._update_experiment_status(experiment_id, 'running')
        
        # Configure threshold assignment
        await self._configure_threshold_assignment(experiment_id)
        
        # Start monitoring task
        asyncio.create_task(self._monitor_experiment(experiment_id))
        
        return {
            'experiment_id': experiment_id,
            'status': 'running',
            'started_at': datetime.utcnow()
        }
    
    async def monitor_experiment_progress(self, experiment_id: str) -> Dict[str, Any]:
        """
        Monitor experiment progress and check for early stopping criteria.
        
        Returns:
            {
                'experiment_id': str,
                'status': str,
                'progress_percentage': float,
                'current_sample_sizes': {
                    'control': int,
                    'test': int
                },
                'preliminary_results': {
                    'control_violation_rate': float,
                    'test_violation_rate': float,
                    'effect_size': float,
                    'p_value': float,
                    'confidence_interval': Tuple[float, float]
                },
                'early_stopping_triggered': bool,
                'early_stopping_reason': Optional[str]
            }
        """
        
        # Get current data
        control_data, test_data = await self._get_experiment_data(experiment_id)
        
        # Calculate current sample sizes
        control_sample_size = len(control_data)
        test_sample_size = len(test_data)
        total_samples = control_sample_size + test_sample_size
        
        # Get required sample size
        experiment_config = await self._get_experiment_config(experiment_id)
        required_sample_size = experiment_config['required_sample_size']
        
        progress_percentage = min(100.0, (total_samples / required_sample_size) * 100)
        
        # Calculate preliminary results
        preliminary_results = None
        early_stopping_triggered = False
        early_stopping_reason = None
        
        if control_sample_size >= 30 and test_sample_size >= 30:  # Minimum for t-test
            preliminary_results = self._calculate_experiment_results(control_data, test_data)
            
            # Check early stopping criteria
            early_stopping_triggered, early_stopping_reason = self._check_early_stopping_criteria(
                preliminary_results, experiment_config
            )
        
        return {
            'experiment_id': experiment_id,
            'status': experiment_config['status'],
            'progress_percentage': progress_percentage,
            'current_sample_sizes': {
                'control': control_sample_size,
                'test': test_sample_size
            },
            'preliminary_results': preliminary_results,
            'early_stopping_triggered': early_stopping_triggered,
            'early_stopping_reason': early_stopping_reason
        }
    
    def _calculate_required_sample_size(
        self, 
        current_threshold: float, 
        proposed_threshold: float,
        service_type: str
    ) -> Dict[str, float]:
        """Calculate required sample size using power analysis."""
        
        # Estimate effect size based on threshold change
        effect_size = abs(proposed_threshold - current_threshold) / current_threshold
        
        # Use Cohen's d for standardized effect size
        # Small: 0.2, Medium: 0.5, Large: 0.8
        if effect_size < 0.1:
            cohens_d = 0.2  # Small effect
        elif effect_size < 0.3:
            cohens_d = 0.5  # Medium effect
        else:
            cohens_d = 0.8  # Large effect
        
        # Calculate sample size using statistical power analysis
        # This is a simplified calculation - in practice, use statsmodels.stats.power
        z_alpha = stats.norm.ppf(1 - self.significance_level / 2)  # Two-tailed test
        z_beta = stats.norm.ppf(self.power)
        
        required_sample_size_per_group = (
            2 * ((z_alpha + z_beta) ** 2) / (cohens_d ** 2)
        )
        
        total_required_sample_size = int(required_sample_size_per_group * 2)
        
        return {
            'effect_size': effect_size,
            'cohens_d': cohens_d,
            'required_sample_size_per_group': int(required_sample_size_per_group),
            'required_sample_size': total_required_sample_size,
            'significance_level': self.significance_level,
            'power': self.power
        }
    
    def _calculate_experiment_results(
        self, 
        control_data: List[float], 
        test_data: List[float]
    ) -> Dict[str, Any]:
        """Calculate statistical results for the experiment."""
        
        control_array = np.array(control_data)
        test_array = np.array(test_data)
        
        # Basic statistics
        control_mean = np.mean(control_array)
        test_mean = np.mean(test_array)
        
        # Convert to violation rates (assuming threshold violations)
        control_threshold = np.percentile(control_array, 95)  # Use 95th percentile as threshold
        test_threshold = np.percentile(test_array, 95)
        
        control_violation_rate = np.mean(control_array > control_threshold)
        test_violation_rate = np.mean(test_array > test_threshold)
        
        # Statistical test (Welch's t-test for unequal variances)
        t_stat, p_value = stats.ttest_ind(control_array, test_array, equal_var=False)
        
        # Effect size (Cohen's d)
        pooled_std = np.sqrt((np.std(control_array)**2 + np.std(test_array)**2) / 2)
        cohens_d = (test_mean - control_mean) / pooled_std
        
        # Confidence interval for difference in means
        se_diff = np.sqrt(np.var(control_array)/len(control_array) + np.var(test_array)/len(test_array))
        df = len(control_array) + len(test_array) - 2
        t_critical = stats.t.ppf(1 - self.significance_level/2, df)
        
        mean_diff = test_mean - control_mean
        ci_lower = mean_diff - t_critical * se_diff
        ci_upper = mean_diff + t_critical * se_diff
        
        return {
            'control_mean': control_mean,
            'test_mean': test_mean,
            'control_violation_rate': control_violation_rate,
            'test_violation_rate': test_violation_rate,
            'mean_difference': mean_diff,
            'effect_size': cohens_d,
            'p_value': p_value,
            'confidence_interval': (ci_lower, ci_upper),
            'is_significant': p_value < self.significance_level,
            't_statistic': t_stat,
            'degrees_of_freedom': df
        }
```

## 🎨 **FRONTEND IMPLEMENTATION**

### **Threshold Optimization Dashboard**

```typescript
// web-builder/src/components/builder/ThresholdOptimizationPanel.tsx

'use client';

import React, { useState, useEffect } from 'react';
import { Card, CardContent, CardDescription, CardHeader, CardTitle } from '@/components/ui/card';
import { Button } from '@/components/ui/button';
import { Badge } from '@/components/ui/badge';
import { Progress } from '@/components/ui/progress';
import { Switch } from '@/components/ui/switch';
import { Select, SelectContent, SelectItem, SelectTrigger, SelectValue } from '@/components/ui/select';
import { 
  TrendingUp, 
  TrendingDown, 
  Target, 
  Brain, 
  TestTube2, 
  CheckCircle2, 
  AlertTriangle,
  BarChart3,
  Clock,
  Zap,
  Settings,
  History
} from 'lucide-react';
import {
  LineChart,
  Line,
  AreaChart,
  Area,
  BarChart,
  Bar,
  XAxis,
  YAxis,
  CartesianGrid,
  Tooltip,
  Legend,
  ResponsiveContainer
} from 'recharts';

// Types
interface SLAOptimizationRecommendation {
  service_type: string;
  current_threshold: number;
  recommended_threshold: number;
  optimization_rationale: {
    statistical_basis: string;
    reliability_impact: string;
    achievability_improvement: string;
    business_justification: string;
  };
  predicted_outcomes: {
    expected_violation_rate: number;
    reliability_score_change: number;
    team_stress_reduction: number;
  };
  confidence_score: number;
  implementation_risk: 'low' | 'medium' | 'high';
  rollback_criteria: string[];
}

interface PerformanceAnalysis {
  service_type: string;
  performance_statistics: {
    mean: number;
    median: number;
    percentile_95: number;
    percentile_99: number;
    coefficient_variation: number;
  };
  trend_analysis: {
    direction: 'improving' | 'degrading' | 'stable';
    trend_slope: number;
    seasonal_patterns: Array<any>;
  };
  baseline_performance: {
    baseline_value: number;
    confidence_interval_lower: number;
    confidence_interval_upper: number;
    is_stable: boolean;
  };
}

interface ABTestExperiment {
  experiment_id: string;
  service_type: string;
  control_threshold: number;
  test_threshold: number;
  status: string;
  progress_percentage: number;
  preliminary_results?: {
    effect_size: number;
    p_value: number;
    is_significant: boolean;
  };
}

export const ThresholdOptimizationPanel: React.FC = () => {
  const [activeTab, setActiveTab] = useState<'overview' | 'recommendations' | 'experiments' | 'history'>('overview');
  const [optimizationRecommendations, setOptimizationRecommendations] = useState<SLAOptimizationRecommendation[]>([]);
  const [performanceAnalysis, setPerformanceAnalysis] = useState<PerformanceAnalysis[]>([]);
  const [activeExperiments, setActiveExperiments] = useState<ABTestExperiment[]>([]);
  const [loading, setLoading] = useState(false);
  const [autoOptimization, setAutoOptimization] = useState(false);

  useEffect(() => {
    loadOptimizationData();
  }, []);

  const loadOptimizationData = async () => {
    setLoading(true);
    try {
      // Load recommendations
      const recommendationsResponse = await fetch('/api/v1/sla-optimization/recommendations');
      const recommendations = await recommendationsResponse.json();
      setOptimizationRecommendations(recommendations);

      // Load performance analysis
      const analysisResponse = await fetch('/api/v1/sla-optimization/performance-analysis');
      const analysis = await analysisResponse.json();
      setPerformanceAnalysis(analysis);

      // Load active experiments
      const experimentsResponse = await fetch('/api/v1/sla-optimization/experiments/active');
      const experiments = await experimentsResponse.json();
      setActiveExperiments(experiments);
    } catch (error) {
      console.error('Error loading optimization data:', error);
    } finally {
      setLoading(false);
    }
  };

  const handleThresholdUpdate = async (serviceType: string, newThreshold: number) => {
    try {
      const response = await fetch('/api/v1/sla-optimization/update-threshold', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          service_type: serviceType,
          new_threshold: newThreshold,
          implementation_type: 'immediate'
        })
      });

      if (response.ok) {
        await loadOptimizationData();
      }
    } catch (error) {
      console.error('Error updating threshold:', error);
    }
  };

  const startABTest = async (serviceType: string, proposedThreshold: number) => {
    try {
      const response = await fetch('/api/v1/sla-optimization/experiments', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          service_type: serviceType,
          proposed_threshold: proposedThreshold,
          experiment_duration_days: 14
        })
      });

      if (response.ok) {
        await loadOptimizationData();
      }
    } catch (error) {
      console.error('Error starting A/B test:', error);
    }
  };

  // Render Overview Tab
  const renderOverviewTab = () => (
    <div className="space-y-6">
      {/* Performance Summary Cards */}
      <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4 gap-4">
        <Card>
          <CardHeader className="flex flex-row items-center justify-between space-y-0 pb-2">
            <CardTitle className="text-sm font-medium">Active Services</CardTitle>
            <Target className="h-4 w-4 text-muted-foreground" />
          </CardHeader>
          <CardContent>
            <div className="text-2xl font-bold">{performanceAnalysis.length}</div>
            <p className="text-xs text-muted-foreground">
              SLA-monitored services
            </p>
          </CardContent>
        </Card>

        <Card>
          <CardHeader className="flex flex-row items-center justify-between space-y-0 pb-2">
            <CardTitle className="text-sm font-medium">Optimization Opportunities</CardTitle>
            <Brain className="h-4 w-4 text-muted-foreground" />
          </CardHeader>
          <CardContent>
            <div className="text-2xl font-bold">{optimizationRecommendations.length}</div>
            <p className="text-xs text-muted-foreground">
              ML-generated recommendations
            </p>
          </CardContent>
        </Card>

        <Card>
          <CardHeader className="flex flex-row items-center justify-between space-y-0 pb-2">
            <CardTitle className="text-sm font-medium">Active Experiments</CardTitle>
            <TestTube2 className="h-4 w-4 text-muted-foreground" />
          </CardHeader>
          <CardContent>
            <div className="text-2xl font-bold">{activeExperiments.length}</div>
            <p className="text-xs text-muted-foreground">
              A/B tests running
            </p>
          </CardContent>
        </Card>

        <Card>
          <CardHeader className="flex flex-row items-center justify-between space-y-0 pb-2">
            <CardTitle className="text-sm font-medium">Auto Optimization</CardTitle>
            <Zap className="h-4 w-4 text-muted-foreground" />
          </CardHeader>
          <CardContent>
            <div className="flex items-center space-x-2">
              <Switch 
                checked={autoOptimization} 
                onCheckedChange={setAutoOptimization}
              />
              <span className="text-sm">
                {autoOptimization ? 'Enabled' : 'Disabled'}
              </span>
            </div>
          </CardContent>
        </Card>
      </div>

      {/* Performance Trends Chart */}
      <Card>
        <CardHeader>
          <CardTitle className="flex items-center gap-2">
            <BarChart3 className="h-5 w-5" />
            SLA Performance Trends
          </CardTitle>
          <CardDescription>
            Historical performance and threshold effectiveness
          </CardDescription>
        </CardHeader>
        <CardContent>
          <div className="h-80">
            <ResponsiveContainer width="100%" height="100%">
              <AreaChart data={generateTrendData()}>
                <CartesianGrid strokeDasharray="3 3" />
                <XAxis dataKey="date" />
                <YAxis />
                <Tooltip />
                <Legend />
                <Area
                  type="monotone"
                  dataKey="violation_rate"
                  stackId="1"
                  stroke="#8884d8"
                  fill="#8884d8"
                  name="Violation Rate (%)"
                />
                <Area
                  type="monotone"
                  dataKey="performance_score"
                  stackId="2"
                  stroke="#82ca9d"
                  fill="#82ca9d"
                  name="Performance Score"
                />
              </AreaChart>
            </ResponsiveContainer>
          </div>
        </CardContent>
      </Card>
    </div>
  );

  // Render Recommendations Tab
  const renderRecommendationsTab = () => (
    <div className="space-y-6">
      {optimizationRecommendations.map((recommendation) => (
        <Card key={recommendation.service_type}>
          <CardHeader>
            <div className="flex items-center justify-between">
              <div>
                <CardTitle className="flex items-center gap-2">
                  <Target className="h-5 w-5" />
                  {recommendation.service_type.replace('_', ' ').toUpperCase()}
                </CardTitle>
                <CardDescription>
                  ML-optimized threshold recommendation
                </CardDescription>
              </div>
              <div className="flex items-center gap-2">
                <Badge 
                  variant={
                    recommendation.implementation_risk === 'low' ? 'default' :
                    recommendation.implementation_risk === 'medium' ? 'secondary' : 'destructive'
                  }
                >
                  {recommendation.implementation_risk} risk
                </Badge>
                <Badge variant="outline">
                  {(recommendation.confidence_score * 100).toFixed(1)}% confidence
                </Badge>
              </div>
            </div>
          </CardHeader>
          <CardContent className="space-y-4">
            {/* Threshold Comparison */}
            <div className="grid grid-cols-1 md:grid-cols-3 gap-4">
              <div className="text-center p-4 bg-muted rounded-lg">
                <div className="text-sm text-muted-foreground">Current</div>
                <div className="text-2xl font-bold">{recommendation.current_threshold}s</div>
              </div>
              <div className="flex items-center justify-center">
                <TrendingUp className="h-6 w-6 text-green-500" />
              </div>
              <div className="text-center p-4 bg-green-50 rounded-lg">
                <div className="text-sm text-muted-foreground">Recommended</div>
                <div className="text-2xl font-bold text-green-600">
                  {recommendation.recommended_threshold}s
                </div>
              </div>
            </div>

            {/* Predicted Outcomes */}
            <div className="grid grid-cols-1 md:grid-cols-3 gap-4">
              <div className="text-center">
                <div className="text-sm text-muted-foreground">Expected Violation Rate</div>
                <div className="text-lg font-semibold">
                  {(recommendation.predicted_outcomes.expected_violation_rate * 100).toFixed(1)}%
                </div>
              </div>
              <div className="text-center">
                <div className="text-sm text-muted-foreground">Reliability Change</div>
                <div className="text-lg font-semibold text-green-600">
                  +{(recommendation.predicted_outcomes.reliability_score_change * 100).toFixed(1)}%
                </div>
              </div>
              <div className="text-center">
                <div className="text-sm text-muted-foreground">Stress Reduction</div>
                <div className="text-lg font-semibold text-blue-600">
                  {(recommendation.predicted_outcomes.team_stress_reduction * 100).toFixed(1)}%
                </div>
              </div>
            </div>

            {/* Rationale */}
            <div className="space-y-2">
              <h4 className="font-semibold">Optimization Rationale</h4>
              <div className="text-sm space-y-1">
                <p><strong>Statistical Basis:</strong> {recommendation.optimization_rationale.statistical_basis}</p>
                <p><strong>Business Impact:</strong> {recommendation.optimization_rationale.business_justification}</p>
              </div>
            </div>

            {/* Actions */}
            <div className="flex gap-2">
              <Button 
                onClick={() => handleThresholdUpdate(recommendation.service_type, recommendation.recommended_threshold)}
                className="flex-1"
              >
                <CheckCircle2 className="h-4 w-4 mr-2" />
                Apply Immediately
              </Button>
              <Button 
                variant="outline"
                onClick={() => startABTest(recommendation.service_type, recommendation.recommended_threshold)}
                className="flex-1"
              >
                <TestTube2 className="h-4 w-4 mr-2" />
                A/B Test First
              </Button>
            </div>
          </CardContent>
        </Card>
      ))}
    </div>
  );

  // Render Experiments Tab
  const renderExperimentsTab = () => (
    <div className="space-y-6">
      {activeExperiments.length === 0 ? (
        <Card>
          <CardContent className="text-center py-8">
            <TestTube2 className="h-12 w-12 mx-auto text-muted-foreground mb-4" />
            <h3 className="text-lg font-semibold mb-2">No Active Experiments</h3>
            <p className="text-muted-foreground">
              Start an A/B test from the recommendations tab to validate threshold changes
            </p>
          </CardContent>
        </Card>
      ) : (
        activeExperiments.map((experiment) => (
          <Card key={experiment.experiment_id}>
            <CardHeader>
              <div className="flex items-center justify-between">
                <CardTitle className="flex items-center gap-2">
                  <TestTube2 className="h-5 w-5" />
                  {experiment.service_type.replace('_', ' ').toUpperCase()} A/B Test
                </CardTitle>
                <Badge variant={experiment.status === 'running' ? 'default' : 'secondary'}>
                  {experiment.status}
                </Badge>
              </div>
            </CardHeader>
            <CardContent className="space-y-4">
              {/* Progress */}
              <div>
                <div className="flex justify-between text-sm mb-2">
                  <span>Progress</span>
                  <span>{experiment.progress_percentage.toFixed(1)}%</span>
                </div>
                <Progress value={experiment.progress_percentage} />
              </div>

              {/* Threshold Comparison */}
              <div className="grid grid-cols-2 gap-4">
                <div className="text-center p-3 bg-blue-50 rounded-lg">
                  <div className="text-sm text-muted-foreground">Control (Current)</div>
                  <div className="text-xl font-bold text-blue-600">
                    {experiment.control_threshold}s
                  </div>
                </div>
                <div className="text-center p-3 bg-green-50 rounded-lg">
                  <div className="text-sm text-muted-foreground">Test (Proposed)</div>
                  <div className="text-xl font-bold text-green-600">
                    {experiment.test_threshold}s
                  </div>
                </div>
              </div>

              {/* Preliminary Results */}
              {experiment.preliminary_results && (
                <div className="space-y-2">
                  <h4 className="font-semibold">Preliminary Results</h4>
                  <div className="grid grid-cols-3 gap-4 text-center">
                    <div>
                      <div className="text-sm text-muted-foreground">Effect Size</div>
                      <div className="font-semibold">
                        {experiment.preliminary_results.effect_size.toFixed(3)}
                      </div>
                    </div>
                    <div>
                      <div className="text-sm text-muted-foreground">P-Value</div>
                      <div className="font-semibold">
                        {experiment.preliminary_results.p_value.toFixed(4)}
                      </div>
                    </div>
                    <div>
                      <div className="text-sm text-muted-foreground">Significant</div>
                      <div className="flex items-center justify-center">
                        {experiment.preliminary_results.is_significant ? (
                          <CheckCircle2 className="h-4 w-4 text-green-500" />
                        ) : (
                          <Clock className="h-4 w-4 text-orange-500" />
                        )}
                      </div>
                    </div>
                  </div>
                </div>
              )}
            </CardContent>
          </Card>
        ))
      )}
    </div>
  );

  const generateTrendData = () => {
    // Mock data for demonstration
    const dates = [];
    const baseDate = new Date();
    for (let i = 29; i >= 0; i--) {
      const date = new Date(baseDate);
      date.setDate(date.getDate() - i);
      dates.push({
        date: date.toISOString().split('T')[0],
        violation_rate: Math.random() * 10 + 2,
        performance_score: Math.random() * 20 + 80
      });
    }
    return dates;
  };

  return (
    <div className="space-y-6">
      {/* Header */}
      <div className="flex items-center justify-between">
        <div>
          <h2 className="text-2xl font-bold">SLA Threshold Optimization</h2>
          <p className="text-muted-foreground">
            ML-powered automatic optimization of SLA thresholds
          </p>
        </div>
        <div className="flex gap-2">
          <Button variant="outline" onClick={loadOptimizationData}>
            Refresh Data
          </Button>
          <Button>
            <Settings className="h-4 w-4 mr-2" />
            Configure
          </Button>
        </div>
      </div>

      {/* Navigation Tabs */}
      <div className="flex border-b">
        <Button 
          variant={activeTab === 'overview' ? 'default' : 'ghost'}
          onClick={() => setActiveTab('overview')}
        >
          <BarChart3 className="h-4 w-4 mr-2" />
          Overview
        </Button>
        <Button 
          variant={activeTab === 'recommendations' ? 'default' : 'ghost'}
          onClick={() => setActiveTab('recommendations')}
        >
          <Brain className="h-4 w-4 mr-2" />
          Recommendations
        </Button>
        <Button 
          variant={activeTab === 'experiments' ? 'default' : 'ghost'}
          onClick={() => setActiveTab('experiments')}
        >
          <TestTube2 className="h-4 w-4 mr-2" />
          A/B Tests
        </Button>
        <Button 
          variant={activeTab === 'history' ? 'default' : 'ghost'}
          onClick={() => setActiveTab('history')}
        >
          <History className="h-4 w-4 mr-2" />
          History
        </Button>
      </div>

      {/* Tab Content */}
      {loading ? (
        <div className="flex justify-center py-8">
          <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-gray-900"></div>
        </div>
      ) : (
        <>
          {activeTab === 'overview' && renderOverviewTab()}
          {activeTab === 'recommendations' && renderRecommendationsTab()}
          {activeTab === 'experiments' && renderExperimentsTab()}
          {activeTab === 'history' && <div>History tab content...</div>}
        </>
      )}
    </div>
  );
};
```

## 🔗 **API ENDPOINTS**

### **Complete REST API Specification**

```python
# backend/app/api/v1/endpoints/sla_optimization.py

from typing import List, Dict, Any, Optional
from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks
from sqlalchemy.orm import Session
from pydantic import BaseModel, Field
from datetime import datetime

from app.core.database import get_db
from app.services.sla_performance_analysis_service import SLAPerformanceAnalysisService
from app.services.sla_threshold_optimizer import SLAThresholdOptimizer
from app.services.sla_ab_testing_service import SLAABTestingService
from app.services.threshold_management_service import ThresholdManagementService

router = APIRouter(prefix="/sla-optimization", tags=["SLA Optimization"])

# Pydantic Models
class PerformanceAnalysisRequest(BaseModel):
    service_type: str
    analysis_period_days: int = Field(default=30, ge=7, le=365)
    include_seasonal_patterns: bool = True
    confidence_level: float = Field(default=0.95, ge=0.8, le=0.99)

class ThresholdOptimizationRequest(BaseModel):
    service_type: str
    optimization_strategy: str = Field(default="balanced")  # balanced, aggressive, conservative
    business_priority: str = Field(default="reliability")  # reliability, efficiency, cost

class ThresholdUpdateRequest(BaseModel):
    service_type: str
    new_threshold: float = Field(gt=0)
    implementation_type: str = Field(default="immediate")  # immediate, scheduled, ab_test
    rollback_criteria: Optional[List[str]] = None
    scheduled_time: Optional[datetime] = None

class ABTestCreationRequest(BaseModel):
    service_type: str
    proposed_threshold: float = Field(gt=0)
    experiment_duration_days: int = Field(default=14, ge=3, le=60)
    traffic_split: float = Field(default=0.5, ge=0.1, le=0.9)
    significance_level: float = Field(default=0.05, ge=0.01, le=0.1)

# Endpoints

@router.post("/performance-analysis")
async def analyze_performance(
    request: PerformanceAnalysisRequest,
    db: Session = Depends(get_db)
) -> Dict[str, Any]:
    """
    Analyze historical performance for a service type.
    
    Returns comprehensive performance statistics, trends, and baselines.
    """
    try:
        analysis_service = SLAPerformanceAnalysisService()
        result = await analysis_service.analyze_service_performance(
            service_type=request.service_type,
            analysis_period_days=request.analysis_period_days
        )
        return result
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")

@router.get("/performance-analysis")
async def get_all_performance_analysis(
    db: Session = Depends(get_db)
) -> List[Dict[str, Any]]:
    """Get performance analysis for all monitored services."""
    try:
        analysis_service = SLAPerformanceAnalysisService()
        # Get all monitored service types
        service_types = await analysis_service.get_monitored_service_types()
        
        results = []
        for service_type in service_types:
            try:
                result = await analysis_service.get_cached_analysis(service_type)
                if result:
                    results.append(result)
            except Exception as e:
                # Log error but continue with other services
                print(f"Error getting analysis for {service_type}: {e}")
                
        return results
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get performance analysis: {str(e)}")

@router.post("/recommendations")
async def generate_optimization_recommendations(
    request: ThresholdOptimizationRequest,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db)
) -> Dict[str, Any]:
    """
    Generate ML-based threshold optimization recommendations.
    """
    try:
        # Get performance analysis
        analysis_service = SLAPerformanceAnalysisService()
        performance_analysis = await analysis_service.analyze_service_performance(
            service_type=request.service_type
        )
        
        # Generate optimization recommendation
        optimizer = SLAThresholdOptimizer()
        recommendation = await optimizer.optimize_thresholds(performance_analysis)
        
        # Schedule background tasks for model improvement
        background_tasks.add_task(
            optimizer.update_model_performance_metrics,
            request.service_type,
            recommendation
        )
        
        return recommendation
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Optimization failed: {str(e)}")

@router.get("/recommendations")
async def get_all_recommendations(
    db: Session = Depends(get_db)
) -> List[Dict[str, Any]]:
    """Get optimization recommendations for all services."""
    try:
        analysis_service = SLAPerformanceAnalysisService()
        optimizer = SLAThresholdOptimizer()
        
        service_types = await analysis_service.get_monitored_service_types()
        recommendations = []
        
        for service_type in service_types:
            try:
                # Check if recommendation exists and is recent
                cached_recommendation = await optimizer.get_cached_recommendation(service_type)
                
                if cached_recommendation and await optimizer.is_recommendation_valid(cached_recommendation):
                    recommendations.append(cached_recommendation)
                else:
                    # Generate fresh recommendation
                    performance_analysis = await analysis_service.get_cached_analysis(service_type)
                    if performance_analysis:
                        recommendation = await optimizer.optimize_thresholds(performance_analysis)
                        recommendations.append(recommendation)
            except Exception as e:
                print(f"Error generating recommendation for {service_type}: {e}")
                
        return recommendations
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get recommendations: {str(e)}")

@router.post("/update-threshold")
async def update_threshold(
    request: ThresholdUpdateRequest,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db)
) -> Dict[str, Any]:
    """
    Update SLA threshold with impact assessment and rollback capability.
    """
    try:
        threshold_manager = ThresholdManagementService()
        
        # Validate new threshold
        validation_result = await threshold_manager.validate_threshold_change(
            service_type=request.service_type,
            new_threshold=request.new_threshold
        )
        
        if not validation_result['is_valid']:
            raise HTTPException(
                status_code=400, 
                detail=f"Invalid threshold: {validation_result['reason']}"
            )
        
        # Perform impact assessment
        impact_assessment = await threshold_manager.assess_change_impact(
            service_type=request.service_type,
            current_threshold=validation_result['current_threshold'],
            new_threshold=request.new_threshold
        )
        
        if request.implementation_type == "immediate":
            # Apply change immediately
            update_result = await threshold_manager.update_threshold_immediate(
                service_type=request.service_type,
                new_threshold=request.new_threshold,
                rollback_criteria=request.rollback_criteria or [],
                impact_assessment=impact_assessment
            )
            
            # Schedule monitoring task
            background_tasks.add_task(
                threshold_manager.monitor_threshold_change,
                update_result['change_id']
            )
            
        elif request.implementation_type == "scheduled":
            if not request.scheduled_time:
                raise HTTPException(status_code=400, detail="Scheduled time required for scheduled implementation")
                
            update_result = await threshold_manager.schedule_threshold_change(
                service_type=request.service_type,
                new_threshold=request.new_threshold,
                scheduled_time=request.scheduled_time,
                rollback_criteria=request.rollback_criteria or []
            )
        else:
            raise HTTPException(status_code=400, detail="Invalid implementation type")
        
        return {
            "success": True,
            "change_id": update_result['change_id'],
            "implementation_type": request.implementation_type,
            "impact_assessment": impact_assessment,
            "rollback_capability": True
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Threshold update failed: {str(e)}")

@router.post("/rollback-threshold")
async def rollback_threshold(
    change_id: str,
    reason: str,
    db: Session = Depends(get_db)
) -> Dict[str, Any]:
    """
    Rollback a threshold change to previous value.
    """
    try:
        threshold_manager = ThresholdManagementService()
        
        rollback_result = await threshold_manager.rollback_threshold_change(
            change_id=change_id,
            rollback_reason=reason
        )
        
        return {
            "success": True,
            "rollback_id": rollback_result['rollback_id'],
            "previous_threshold": rollback_result['previous_threshold'],
            "rollback_time": rollback_result['rollback_time']
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Rollback failed: {str(e)}")

@router.post("/experiments")
async def create_ab_test(
    request: ABTestCreationRequest,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db)
) -> Dict[str, Any]:
    """
    Create a new A/B test for threshold optimization.
    """
    try:
        ab_testing_service = SLAABTestingService()
        
        # Get current threshold
        threshold_manager = ThresholdManagementService()
        current_threshold = await threshold_manager.get_current_threshold(request.service_type)
        
        # Create experiment
        experiment = await ab_testing_service.create_threshold_experiment(
            service_type=request.service_type,
            current_threshold=current_threshold,
            proposed_threshold=request.proposed_threshold,
            experiment_duration_days=request.experiment_duration_days,
            traffic_split=request.traffic_split
        )
        
        # Start experiment
        start_result = await ab_testing_service.start_experiment(experiment['experiment_id'])
        
        # Schedule monitoring
        background_tasks.add_task(
            ab_testing_service.monitor_experiment_lifecycle,
            experiment['experiment_id']
        )
        
        return {
            "success": True,
            "experiment": experiment,
            "start_result": start_result
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Experiment creation failed: {str(e)}")

@router.get("/experiments/active")
async def get_active_experiments(
    db: Session = Depends(get_db)
) -> List[Dict[str, Any]]:
    """Get all currently active A/B test experiments."""
    try:
        ab_testing_service = SLAABTestingService()
        
        active_experiments = await ab_testing_service.get_active_experiments()
        
        # Enrich with current progress
        enriched_experiments = []
        for experiment in active_experiments:
            progress = await ab_testing_service.monitor_experiment_progress(
                experiment['experiment_id']
            )
            enriched_experiments.append({
                **experiment,
                **progress
            })
            
        return enriched_experiments
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get active experiments: {str(e)}")

@router.get("/experiments/{experiment_id}")
async def get_experiment_details(
    experiment_id: str,
    db: Session = Depends(get_db)
) -> Dict[str, Any]:
    """Get detailed information about a specific experiment."""
    try:
        ab_testing_service = SLAABTestingService()
        
        experiment_details = await ab_testing_service.get_experiment_details(experiment_id)
        progress = await ab_testing_service.monitor_experiment_progress(experiment_id)
        
        return {
            **experiment_details,
            **progress
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get experiment details: {str(e)}")

@router.post("/experiments/{experiment_id}/stop")
async def stop_experiment(
    experiment_id: str,
    reason: str,
    db: Session = Depends(get_db)
) -> Dict[str, Any]:
    """Stop a running A/B test experiment."""
    try:
        ab_testing_service = SLAABTestingService()
        
        stop_result = await ab_testing_service.stop_experiment(
            experiment_id=experiment_id,
            stop_reason=reason
        )
        
        return {
            "success": True,
            "experiment_id": experiment_id,
            "stop_result": stop_result
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to stop experiment: {str(e)}")

@router.get("/history/threshold-changes")
async def get_threshold_change_history(
    service_type: Optional[str] = None,
    limit: int = 50,
    db: Session = Depends(get_db)
) -> List[Dict[str, Any]]:
    """Get historical threshold changes."""
    try:
        threshold_manager = ThresholdManagementService()
        
        history = await threshold_manager.get_threshold_change_history(
            service_type=service_type,
            limit=limit
        )
        
        return history
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get threshold history: {str(e)}")

@router.get("/metrics/optimization-effectiveness")
async def get_optimization_effectiveness_metrics(
    db: Session = Depends(get_db)
) -> Dict[str, Any]:
    """Get metrics on optimization effectiveness."""
    try:
        optimizer = SLAThresholdOptimizer()
        
        effectiveness_metrics = await optimizer.get_optimization_effectiveness_metrics()
        
        return effectiveness_metrics
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get effectiveness metrics: {str(e)}")

@router.get("/health")
async def health_check() -> Dict[str, str]:
    """Health check endpoint for SLA optimization service."""
    return {
        "status": "healthy",
        "service": "sla-optimization",
        "timestamp": datetime.utcnow().isoformat()
    }
```

## 📋 **DEVELOPMENT TASK BREAKDOWN**

### **Phase 1: Foundation & Data Analysis (Days 1-3)**

#### **Task 1.1: Database Schema Setup**
- [ ] Create migration files for new SLA optimization tables
- [ ] Update existing SLA models with optimization fields
- [ ] Set up database indexes for performance
- [ ] Create data validation constraints

**Files to create/modify:**
- `backend/alembic/versions/add_sla_optimization_tables.py`
- `backend/app/models/sla_optimization.py`
- `backend/app/models/workflow.py` (extend existing)

#### **Task 1.2: Performance Analysis Service**
- [ ] Implement `SLAPerformanceAnalysisService`
- [ ] Add statistical analysis methods
- [ ] Create trend detection algorithms
- [ ] Implement outlier detection
- [ ] Add data quality assessment

**Files to create:**
- `backend/app/services/sla_performance_analysis_service.py`
- `backend/app/core/statistics.py`
- `backend/tests/test_sla_performance_analysis.py`

#### **Task 1.3: Data Collection Pipeline**
- [ ] Enhance existing SLA monitoring to collect optimization data
- [ ] Create performance data aggregation jobs
- [ ] Implement data quality monitoring
- [ ] Set up data retention policies

**Files to modify:**
- `scripts/automation/sla-monitor.js`
- `backend/app/services/workflow_automation.py`

### **Phase 2: ML Optimization Engine (Days 4-6)**

#### **Task 2.1: Threshold Optimizer Service**
- [ ] Implement `SLAThresholdOptimizer` class
- [ ] Create multi-objective optimization algorithm
- [ ] Build ML model training pipeline
- [ ] Add prediction confidence scoring
- [ ] Implement model versioning and storage

**Files to create:**
- `backend/app/services/sla_threshold_optimizer.py`
- `backend/app/ml/optimization_models.py`
- `backend/ml_models/` (directory for model artifacts)
- `backend/tests/test_sla_threshold_optimizer.py`

#### **Task 2.2: Impact Prediction Service**
- [ ] Create threshold change impact predictor
- [ ] Implement business impact assessment
- [ ] Add risk scoring algorithms
- [ ] Create prediction validation framework

**Files to create:**
- `backend/app/services/threshold_impact_predictor.py`
- `backend/app/ml/impact_models.py`

#### **Task 2.3: ML Model Management**
- [ ] Implement model training automation
- [ ] Create model performance monitoring
- [ ] Add automated retraining pipeline
- [ ] Implement A/B testing for models

**Files to create:**
- `backend/app/services/ml_model_manager.py`
- `backend/app/tasks/model_training_tasks.py`

### **Phase 3: A/B Testing Framework (Days 7-9)**

#### **Task 3.1: A/B Testing Service**
- [ ] Implement `SLAABTestingService`
- [ ] Create experiment configuration system
- [ ] Add statistical significance testing
- [ ] Implement early stopping criteria
- [ ] Create automated result analysis

**Files to create:**
- `backend/app/services/sla_ab_testing_service.py`
- `backend/app/core/statistical_tests.py`
- `backend/tests/test_sla_ab_testing.py`

#### **Task 3.2: Experiment Management**
- [ ] Create experiment lifecycle management
- [ ] Implement traffic splitting logic
- [ ] Add experiment monitoring dashboard
- [ ] Create automated reporting

**Files to create:**
- `backend/app/services/experiment_manager.py`
- `backend/app/tasks/experiment_monitoring_tasks.py`

### **Phase 4: Threshold Management (Days 10-12)**

#### **Task 4.1: Threshold Management Service**
- [ ] Implement `ThresholdManagementService`
- [ ] Create threshold update workflows
- [ ] Add rollback capability
- [ ] Implement change impact assessment
- [ ] Create audit trail system

**Files to create:**
- `backend/app/services/threshold_management_service.py`
- `backend/app/services/threshold_audit_service.py`
- `backend/tests/test_threshold_management.py`

#### **Task 4.2: Integration with Existing SLA System**
- [ ] Extend existing SLA monitoring
- [ ] Maintain backward compatibility
- [ ] Update violation detection logic
- [ ] Preserve existing alert systems

**Files to modify:**
- `backend/app/models/workflow.py`
- `scripts/automation/sla-monitor.js`
- `backend/app/services/workflow_automation.py`

### **Phase 5: API Layer (Days 13-15)**

#### **Task 5.1: REST API Endpoints**
- [ ] Implement all SLA optimization endpoints
- [ ] Add request/response validation
- [ ] Create API documentation
- [ ] Add rate limiting and security

**Files to create:**
- `backend/app/api/v1/endpoints/sla_optimization.py`
- `backend/app/schemas/sla_optimization.py`
- `backend/tests/test_sla_optimization_api.py`

#### **Task 5.2: API Integration Tests**
- [ ] Create comprehensive API test suite
- [ ] Add performance testing
- [ ] Test error handling
- [ ] Validate security measures

**Files to create:**
- `backend/tests/integration/test_sla_optimization_integration.py`
- `backend/tests/performance/test_sla_optimization_performance.py`

### **Phase 6: Frontend Dashboard (Days 16-20)**

#### **Task 6.1: Core Dashboard Components**
- [ ] Create `ThresholdOptimizationPanel` component
- [ ] Implement performance visualization charts
- [ ] Add recommendation display components
- [ ] Create A/B test monitoring interface

**Files to create:**
- `web-builder/src/components/builder/ThresholdOptimizationPanel.tsx`
- `web-builder/src/components/builder/SLAPerformanceTrends.tsx`
- `web-builder/src/components/builder/ThresholdImpactPreview.tsx`
- `web-builder/src/components/builder/ABTestMonitor.tsx`

#### **Task 6.2: Dashboard Integration**
- [ ] Integrate with existing SLA dashboard
- [ ] Add navigation and routing
- [ ] Implement real-time updates
- [ ] Add responsive design

**Files to modify:**
- `web-builder/src/components/builder/WorkflowDebuggingPanel.tsx`
- `web-builder/src/app/(builder)/analytics/page.tsx`

#### **Task 6.3: UI/UX Components**
- [ ] Create threshold adjustment controls
- [ ] Add confirmation dialogs
- [ ] Implement progress indicators
- [ ] Create success/error notifications

**Files to create:**
- `web-builder/src/components/builder/ThresholdAdjustmentControl.tsx`
- `web-builder/src/components/builder/OptimizationConfirmDialog.tsx`

### **Phase 7: Testing & Validation (Days 21-25)**

#### **Task 7.1: Comprehensive Testing**
- [ ] Unit tests for all services
- [ ] Integration tests for complete workflows
- [ ] Performance testing under load
- [ ] Security testing for vulnerabilities

**Files to create:**
- `backend/tests/unit/test_*.py` (comprehensive unit tests)
- `backend/tests/integration/test_sla_optimization_workflow.py`
- `tests/e2e/sla-threshold-optimization.spec.ts`

#### **Task 7.2: ML Model Validation**
- [ ] Validate statistical accuracy
- [ ] Test prediction reliability
- [ ] Verify optimization effectiveness
- [ ] Validate A/B testing framework

**Files to create:**
- `backend/tests/ml/test_model_accuracy.py`
- `backend/tests/ml/test_optimization_effectiveness.py`
- `backend/tests/ml/test_ab_testing_validity.py`

#### **Task 7.3: User Acceptance Testing**
- [ ] Create UAT test scenarios
- [ ] Test complete user workflows
- [ ] Validate business requirements
- [ ] Performance benchmarking

### **Phase 8: Deployment & Monitoring (Days 26-30)**

#### **Task 8.1: Production Deployment**
- [ ] Set up production ML model infrastructure
- [ ] Configure monitoring and alerting
- [ ] Implement automated deployment pipeline
- [ ] Create rollback procedures

**Files to create:**
- `deployment/sla-optimization-deployment.yml`
- `monitoring/sla-optimization-dashboard.json`
- `.github/workflows/sla-optimization-deploy.yml`

#### **Task 8.2: Production Monitoring**
- [ ] Set up ML model performance monitoring
- [ ] Create optimization effectiveness dashboards
- [ ] Implement automated alerts
- [ ] Add business metric tracking

**Files to create:**
- `backend/app/monitoring/sla_optimization_monitor.py`
- `scripts/monitoring/optimization-health-check.py`

## 🧪 **TESTING STRATEGY**

### **Testing Pyramid**

```
                 E2E Tests (5%)
               ┌─────────────────┐
               │ Full Workflows  │
               └─────────────────┘
            
         Integration Tests (15%)
       ┌─────────────────────────┐
       │   API + Service + DB    │
       └─────────────────────────┘
    
     Unit Tests (80%)
┌─────────────────────────────────┐
│ Services, Models, Algorithms    │
└─────────────────────────────────┘
```

### **ML-Specific Testing**

#### **Statistical Validation Tests**
- Performance analysis accuracy validation
- Threshold optimization effectiveness testing
- A/B test statistical rigor verification
- Prediction accuracy tracking

#### **Model Testing Framework**
- Cross-validation with historical data
- Bias detection and mitigation
- Model drift monitoring
- Adversarial testing scenarios

### **Performance Testing**

#### **Load Testing Scenarios**
- 1000+ concurrent threshold optimization requests
- High-frequency performance data ingestion
- Large-scale A/B test management
- Real-time dashboard updates under load

#### **Scalability Testing**
- Multi-tenant SLA optimization
- Historical data processing at scale
- ML model training performance
- Database query optimization validation

## 📊 **SUCCESS METRICS**

### **Technical KPIs**

- **Performance Analysis Accuracy**: >95% statistical confidence
- **Threshold Optimization Effectiveness**: 25%+ violation rate reduction
- **Impact Prediction Accuracy**: ±15% of actual outcomes
- **A/B Test Statistical Rigor**: 95% confidence with proper power analysis
- **API Response Times**: <500ms for optimization requests
- **Dashboard Load Time**: <2 seconds initial load

### **Business KPIs**

- **SLA Achievement Rate**: 15%+ improvement
- **Team Productivity**: 20%+ reduction in SLA-related incidents
- **User Adoption**: >80% of DevOps teams using recommendations
- **Optimization Frequency**: Weekly automated optimizations
- **Cost Reduction**: 10%+ reduction in SLA violation overhead

## 🔐 **SECURITY CONSIDERATIONS**

### **Data Security**
- Encrypt sensitive performance data at rest
- Secure API endpoints with authentication
- Audit trail for all threshold changes
- GDPR compliance for performance metrics

### **Access Control**
- Role-based permissions for threshold updates
- Multi-factor authentication for critical changes
- Approval workflows for significant modifications
- Audit logs for all optimization activities

## 🚀 **DEPLOYMENT STRATEGY**

### **Phased Rollout**

#### **Phase 1: Internal Testing (Week 1)**
- Deploy to staging environment
- Internal DevOps team validation
- Performance and security testing
- Bug fixes and optimization

#### **Phase 2: Beta Release (Week 2-3)**
- Limited production deployment
- Selected service types only
- Monitor ML model performance
- Gather user feedback

#### **Phase 3: Full Production (Week 4)**
- Complete feature rollout
- All service types enabled
- Automated optimization active
- Full monitoring and alerting

### **Rollback Plan**
- Immediate threshold rollback capability
- ML model version rollback
- Database schema rollback procedures
- Feature flag-based disabling

## 📚 **DOCUMENTATION REQUIREMENTS**

### **Technical Documentation**
- API specification (OpenAPI/Swagger)
- ML model documentation
- Database schema documentation
- Deployment and configuration guides

### **User Documentation**
- SLA optimization user guide
- A/B testing best practices
- Troubleshooting guide
- FAQ and common scenarios

## 🔄 **MAINTENANCE & EVOLUTION**

### **Continuous Improvement**
- Weekly ML model retraining
- Monthly optimization effectiveness review
- Quarterly algorithm evaluation
- Annual system architecture review

### **Feature Evolution Roadmap**
- Advanced ML algorithms (deep learning)
- Multi-objective optimization with business constraints
- Predictive scaling based on optimization trends
- Integration with external monitoring systems

---

## ✅ **DEVELOPMENT READINESS CHECKLIST**

- [x] **Requirements Analysis Complete**: All acceptance criteria defined
- [x] **Technical Architecture Designed**: Complete system design
- [x] **Database Schema Defined**: All tables and relationships specified
- [x] **API Specification Complete**: All endpoints documented
- [x] **Frontend Components Designed**: Complete UI specification
- [x] **Testing Strategy Defined**: Comprehensive testing approach
- [x] **Task Breakdown Complete**: Detailed work breakdown structure
- [x] **Success Metrics Defined**: Clear KPIs and measurement criteria
- [x] **Security Requirements Specified**: Security and compliance measures
- [x] **Deployment Strategy Planned**: Phased rollout approach

## 🎯 **READY FOR DEVELOPMENT**

**Story 3.5 is now DEVELOPMENT READY** with comprehensive specifications, detailed task breakdown, and clear success criteria. The implementation can begin immediately with confidence in the technical approach and expected outcomes.

<function_calls>
<invoke name="TodoWrite">
<parameter name="todos">[{"content": "Analyze Story 3.5 requirements and current SLA infrastructure", "status": "completed", "id": "1"}, {"content": "Design ML-based threshold optimization architecture", "status": "completed", "id": "2"}, {"content": "Create comprehensive backend API specifications", "status": "completed", "id": "3"}, {"content": "Design frontend threshold optimization dashboard", "status": "completed", "id": "4"}, {"content": "Prepare development-ready task breakdown", "status": "in_progress", "id": "5"}]